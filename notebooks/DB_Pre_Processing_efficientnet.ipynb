{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73faf589",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /home/dolly/.pyenv/versions/3.10.6/envs/YOGi/lib/python3.10/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/dolly/.pyenv/versions/3.10.6/envs/YOGi/lib/python3.10/site-packages (from opencv-python) (1.24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opencv-python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adccf1c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "787d6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import utils, optimizers\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.applications import efficientnet\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbea69",
   "metadata": {},
   "source": [
    "# Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7353e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Garland_Pose',\n",
       " 'Peacock_Pose',\n",
       " 'Training',\n",
       " 'Four-Limbed_Staff',\n",
       " 'Sitting_pose_1_(normal)',\n",
       " 'Testing',\n",
       " 'Staff_Pose_',\n",
       " 'Low_Lunge_pose',\n",
       " 'Standing_big_toe_hold']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '../raw_data'\n",
    "poses_list = os.listdir(root_dir) #Lists all folders in the root directory (raw_data folder)\n",
    "poses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0492539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You should not be seeing this since the upper directory is removed beforehand\n"
     ]
    }
   ],
   "source": [
    "# Creates Testing and Training directories\n",
    "def create_train_val_dirs(root_path):\n",
    "    for pose in poses_list:\n",
    "        if pose == \"Testing\" or pose == \"Training\":\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(os.path.join(root_path, 'Training', pose))\n",
    "            os.makedirs(os.path.join(root_path, 'Testing', pose))\n",
    "            \n",
    "            \n",
    "#Tries to create new directories\n",
    "#Errors if Training/Testing directories already exist - delete them before running this notebook!\n",
    "try:\n",
    "    create_train_val_dirs(root_path=root_dir)\n",
    "except FileExistsError:\n",
    "    print(\"You should not be seeing this since the upper directory is removed beforehand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d900cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
    "    \"\"\"Function to split the data into train/test\n",
    "    First checks that the image exists in the folder - this is a safety net for any images which don't exist that we missed when manually checking images \n",
    "    Creates a list of all images and randomises them\n",
    "    Splits into train test depending on split size (defined below)\n",
    "    Copies all the train images into train folder and vice versa\n",
    "    \"\"\"\n",
    "    \n",
    "    ignore = []\n",
    "    for image in os.listdir(SOURCE_DIR):\n",
    "        if type(cv2.imread(os.path.join(SOURCE_DIR, image))) is type(None):\n",
    "            ignore.append(image)\n",
    "    \n",
    "    source_images = [image for image in os.listdir(SOURCE_DIR) if image not in ignore]\n",
    "    # Randomising list\n",
    "    source_images = random.sample(source_images, len(source_images))\n",
    "    \n",
    "    train_images = source_images[:int(SPLIT_SIZE * len(source_images))]\n",
    "    val_images = source_images[int(SPLIT_SIZE * len(source_images)):]\n",
    "    for image in train_images:\n",
    "        shutil.copyfile(os.path.join(SOURCE_DIR, image), os.path.join(TRAINING_DIR, image))\n",
    "    for image in val_images:\n",
    "        shutil.copyfile(os.path.join(SOURCE_DIR, image), os.path.join(VALIDATION_DIR, image))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f976f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 190 images for Garland_Pose in training\n",
      "There are 107 images for Garland_Pose in testing\n",
      "Original Garland_Pose's directory has 236 images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 images for Peacock_Pose in training\n",
      "There are 55 images for Peacock_Pose in testing\n",
      "Original Peacock_Pose's directory has 177 images\n",
      "\n",
      "There are 173 images for Four-Limbed_Staff in training\n",
      "There are 95 images for Four-Limbed_Staff in testing\n",
      "Original Four-Limbed_Staff's directory has 203 images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "Corrupt JPEG data: 21 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 546 images for Sitting_pose_1_(normal) in training\n",
      "There are 306 images for Sitting_pose_1_(normal) in testing\n",
      "Original Sitting_pose_1_(normal)'s directory has 637 images\n",
      "\n",
      "There are 63 images for Staff_Pose_ in training\n",
      "There are 35 images for Staff_Pose_ in testing\n",
      "Original Staff_Pose_'s directory has 72 images\n",
      "\n",
      "There are 217 images for Low_Lunge_pose in training\n",
      "There are 125 images for Low_Lunge_pose in testing\n",
      "Original Low_Lunge_pose's directory has 295 images\n",
      "\n",
      "There are 154 images for Standing_big_toe_hold in training\n",
      "There are 88 images for Standing_big_toe_hold in testing\n",
      "Original Standing_big_toe_hold's directory has 228 images\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_size = 0.7\n",
    "\n",
    "#Loops over all the poses, defines the directories for source/train/test\n",
    "#Splits the data into train/test\n",
    "#Prints how many images are in train/test and in the original directory - sanity check!\n",
    "\n",
    "for pose in poses_list:\n",
    "    if pose == \"Training\" or pose == \"Testing\":\n",
    "        pass\n",
    "    else:\n",
    "        SOURCE_DIR = f\"{root_dir}/{pose}\"\n",
    "        TRAINING_DIR = f\"{root_dir}/Training/{pose}\"\n",
    "        TESTING_DIR = f\"{root_dir}/Testing/{pose}\"    \n",
    "        split_data(SOURCE_DIR, TRAINING_DIR, TESTING_DIR, split_size)\n",
    "\n",
    "        print(f\"There are {len(os.listdir(TRAINING_DIR))} images for {pose} in training\")\n",
    "        print(f\"There are {len(os.listdir(TESTING_DIR))} images for {pose} in testing\")\n",
    "\n",
    "        print(f\"Original {pose}'s directory has {len(os.listdir(SOURCE_DIR))} images\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e20bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These variables can be changes, excluding train_dir\n",
    "\n",
    "train_dir = \"../raw_data/Training\"\n",
    "img_height, img_width = 256, 256\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4be94b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1157 images belonging to 7 classes.\n",
      "Found 286 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "#Splits into train_generator and validation_generator\n",
    "#This bulk uploads the images\n",
    "#Creates target (y) for us!\n",
    "\n",
    "#Play around with the interpolation argument - bicubic, lanczos??? \n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                    vertical_flip=True,\n",
    "                                    validation_split=0.2) # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                    train_dir,\n",
    "                                    target_size=(img_height, img_width),\n",
    "                                    batch_size=batch_size,\n",
    "                                    class_mode='categorical',\n",
    "                                    subset='training',\n",
    "                                    keep_aspect_ratio=True,\n",
    "                                    interpolation='lanczos') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "                                    train_dir, # same directory as training data\n",
    "                                    target_size=(img_height, img_width),\n",
    "                                    batch_size=batch_size,\n",
    "                                    class_mode='categorical',\n",
    "                                    subset='validation',\n",
    "                                    keep_aspect_ratio=True,\n",
    "                                    interpolation='lanczos') # set as validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f530f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4675f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# Model needs building + transfer learning  \n",
    "...\n",
    "def initialize_model():\n",
    "    base_model = efficientnet.EfficientNetB0(\n",
    "                        include_top=False,\n",
    "                        weights='imagenet',\n",
    "                        input_shape=(img_height, img_width, 3),\n",
    "                        classifier_activation='softmax')\n",
    "    \n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model = models.Sequential([ \n",
    "        base_model,\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(500, activation=\"relu\"),\n",
    "        layers.Dense(7, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    opt = optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy']) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fb61e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63383da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb0 (Functional)  (None, 8, 8, 1280)       4049571   \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 81920)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 500)               40960500  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 83)                41583     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,051,654\n",
      "Trainable params: 41,002,083\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12d35456",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6636f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "36/36 [==============================] - 28s 784ms/step - loss: 1.9057 - accuracy: 0.3307 - val_loss: 1.9142 - val_accuracy: 0.3828\n",
      "Epoch 2/2\n",
      "36/36 [==============================] - 29s 787ms/step - loss: 2.0593 - accuracy: 0.3049 - val_loss: 1.8824 - val_accuracy: 0.3711\n"
     ]
    }
   ],
   "source": [
    "#fit model - fit on train_generator (both X and y) and the validation data is validation_generator\n",
    "history = model.fit(\n",
    "                train_generator,\n",
    "                steps_per_epoch = train_generator.samples // batch_size,\n",
    "                validation_data = validation_generator, \n",
    "                validation_steps = validation_generator.samples // batch_size,\n",
    "                epochs = 2,\n",
    "                callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3784a400",
   "metadata": {},
   "source": [
    "# Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has been paused and probably won't be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread(f\"{file_path}/{file_list[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec4d62e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Detected :  2\n"
     ]
    }
   ],
   "source": [
    "# Reading the Image\n",
    "image = cv2.imread(f\"{file_path}/{file_list[3]}\")\n",
    "\n",
    "# initialize the HOG descriptor\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "(humans, _) = hog.detectMultiScale(image, winStride=(3,3),\n",
    "                                padding=(32, 32), \n",
    "                               scale=1.01)\n",
    "\n",
    "# getting no. of human detected\n",
    "print('Human Detected : ', len(humans))\n",
    "\n",
    "# loop over all detected humans\n",
    "for (x, y, w, h) in humans:\n",
    "   pad_w, pad_h = int(0.15 * w), int(0.01 * h)\n",
    "   cv2.rectangle(image, (x + pad_w, y + pad_h), (x + w - pad_w, y + h - pad_h), (0, 255, 0), 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1bf615f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the output image\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(25)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07339aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "170031fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"{file_path}/{file_list[3]}\"\n",
    "photo = cv2.imread(filepath)\n",
    "photo_grey = cv2.cvtColor(photo, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06134bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the HOG person\n",
    "# detector\n",
    "\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "   \n",
    "# Reading the Image\n",
    "image = cv2.imread(filepath)\n",
    "   \n",
    "# Detecting all the regions in the \n",
    "# Image that has a pedestrians inside it\n",
    "(regions, _) = hog.detectMultiScale(image, \n",
    "                                    winStride=(8, 8),\n",
    "                                    padding=(16, 16),\n",
    "                                    scale=1.15)\n",
    "   \n",
    "# Drawing the regions in the Image\n",
    "for (x, y, w, h) in regions:\n",
    "    cv2.rectangle(image, (x, y), \n",
    "                  (x + w, y + h), \n",
    "                  (0, 0, 255), 2)\n",
    "    \n",
    "#Showing the output Image\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)\n",
    "   \n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
