{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.applications import efficientnet\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import utils, optimizers\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YOGI_Ground_Truths']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '../raw_data'\n",
    "img_height, img_width = 128, 128\n",
    "batch_size = 32\n",
    "poses_list = os.listdir(root_dir) #Lists all folders in the root directory (raw_data folder)\n",
    "poses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@3426.050] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mwhile\u001b[39;00m(\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     20\u001b[0m     ret, frame \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[0;32m---> 21\u001b[0m     gray \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(frame, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2GRAY)\n\u001b[1;32m     22\u001b[0m     cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m'\u001b[39m,gray)\n\u001b[1;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m1\u001b[39m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "# VIDEO FEED\n",
    "#cap = cv2.VideoCapture(0)\n",
    "\n",
    "# with mp_pose.Pose(\n",
    "#     min_detection_confidence=0.5,\n",
    "#     min_tracking_confidence=0.5) as pose:\n",
    "#   while cap.isOpened():\n",
    "#     success, image = cap.read()\n",
    "#     if not success:\n",
    "#       print(\"Ignoring empty camera frame.\")\n",
    "#       # If loading a video, use 'break' instead of 'continue'.\n",
    "#       continue\n",
    "\n",
    "#     # To improve performance, optionally mark the image as not writeable to\n",
    "#     # pass by reference.\n",
    "#     image.flags.writeable = False\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "# while(True):\n",
    "#     ret, frame = cap.read()\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     cv2.imshow('frame',gray)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "#VIDEO FEED\n",
    "cap = cv2.VideoCapture(0)\n",
    "assert webcam.isOpened()\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Mediapipe Feed', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For static images:\n",
    "# IMAGE_FILES = []\n",
    "# BG_COLOR = (192, 192, 192) # gray\n",
    "# with mp_pose.Pose(\n",
    "#     static_image_mode=True,\n",
    "#     model_complexity=2,\n",
    "#     enable_segmentation=True,\n",
    "#     min_detection_confidence=0.5) as pose:\n",
    "#   for idx, file in enumerate(IMAGE_FILES):\n",
    "#     image = cv2.imread(file)\n",
    "#     image_height, image_width, _ = image.shape\n",
    "#     # Convert the BGR image to RGB before processing.\n",
    "#     results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "#     if not results.pose_landmarks:\n",
    "#       continue\n",
    "#     print(\n",
    "#         f'Nose coordinates: ('\n",
    "#         f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].x * image_width}, '\n",
    "#         f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].y * image_height})'\n",
    "#     )\n",
    "\n",
    "#     annotated_image = image.copy()\n",
    "#     # Draw segmentation on the image.\n",
    "#     # To improve segmentation around boundaries, consider applying a joint\n",
    "#     # bilateral filter to \"results.segmentation_mask\" with \"image\".\n",
    "#     condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "#     bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "#     bg_image[:] = BG_COLOR\n",
    "#     annotated_image = np.where(condition, annotated_image, bg_image)\n",
    "#     # Draw pose landmarks on the image.\n",
    "#     mp_drawing.draw_landmarks(\n",
    "#         annotated_image,\n",
    "#         results.pose_landmarks,\n",
    "#         mp_pose.POSE_CONNECTIONS,\n",
    "#         landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "#     cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n",
    "#     # Plot pose world landmarks.\n",
    "#     mp_drawing.plot_landmarks(\n",
    "#         results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@295.479] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@295.479] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            print(landmarks)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'landmarks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mlen\u001b[39m(landmarks)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'landmarks' is not defined"
     ]
    }
   ],
   "source": [
    "len(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lndmrk in mp_pose.PoseLandmark:\n",
    "    print(lndmrk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoulder, elbow, wrist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_angle(shoulder, elbow, wrist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yogi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
