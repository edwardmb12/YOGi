{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from cv2 import imshow\n",
    "from math import atan2, degrees, fabs\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.11):\n",
    "    \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "    Args:\n",
    "        keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "        the keypoint coordinates and scores returned from the MoveNet model.\n",
    "        height: height of the image in pixels.\n",
    "        width: width of the image in pixels.\n",
    "        keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "        visualized.\n",
    "\n",
    "    Returns:\n",
    "        A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "        * the coordinates of all keypoints of all detected entities;\n",
    "        * the coordinates of all skeleton edges of all detected entities;\n",
    "        * the colors in which the edges should be plotted.\n",
    "    \"\"\"\n",
    "    keypoints_all = []\n",
    "    keypoint_edges_all = []\n",
    "    edge_colors = []\n",
    "    num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "    for idx in range(num_instances):\n",
    "        kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "        kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "        kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "        kpts_absolute_xy = np.stack(\n",
    "            [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "        kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "            kpts_scores > keypoint_threshold, :]\n",
    "        keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "        for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "            if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "            kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "                x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "                y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "                x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "                y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "                line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "                keypoint_edges_all.append(line_seg)\n",
    "                edge_colors.append(color)\n",
    "    if keypoints_all:\n",
    "        keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "    else:\n",
    "        keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "    if keypoint_edges_all:\n",
    "        edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "    else:\n",
    "        edges_xy = np.zeros((0, 2, 2))\n",
    "    return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "    \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "    Args:\n",
    "    image: A numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "      of the crop region in normalized coordinates (see the init_crop_region\n",
    "      function below for more detail). If provided, this function will also\n",
    "      draw the bounding box on the image.\n",
    "    output_image_height: An integer indicating the height of the output image.\n",
    "      Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "    Returns:\n",
    "    A numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "    \"\"\"\n",
    "    height, width, channel = image.shape\n",
    "    aspect_ratio = float(width) / height\n",
    "    fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "    # To remove the huge white borders\n",
    "    fig.tight_layout(pad=0)\n",
    "    ax.margins(0)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    plt.axis('off')\n",
    "\n",
    "    im = ax.imshow(image)\n",
    "    line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "    ax.add_collection(line_segments)\n",
    "    # Turn off tick labels\n",
    "    scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "    (keypoint_locs, keypoint_edges,\n",
    "    edge_colors) = _keypoints_and_edges_for_display(\n",
    "        keypoints_with_scores, height, width)\n",
    "\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "    if keypoint_edges.shape[0]:\n",
    "        line_segments.set_segments(keypoint_edges)\n",
    "        line_segments.set_color(edge_colors)\n",
    "    if keypoint_locs.shape[0]:\n",
    "        scat.set_offsets(keypoint_locs)\n",
    "\n",
    "    if crop_region is not None:\n",
    "        xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "        ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "        rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "        rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin,ymin),rec_width,rec_height,\n",
    "            linewidth=1,edgecolor='b',facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    image_from_plot = image_from_plot.reshape(\n",
    "        fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    plt.close(fig)\n",
    "    if output_image_height is not None:\n",
    "        output_image_width = int(output_image_height / height * width)\n",
    "        image_from_plot = cv2.resize(\n",
    "            image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "            interpolation=cv2.INTER_CUBIC)\n",
    "    return image_from_plot\n",
    "\n",
    "def plot_on_image(image, keypoints_with_scores):\n",
    "\n",
    "    # Visualize the predictions with image.\n",
    "    display_image = tf.expand_dims(image, axis=0)\n",
    "    display_image = tf.cast(tf.image.resize_with_pad(\n",
    "        display_image, 1280, 1280), dtype=tf.int32)\n",
    "    output_overlay = draw_prediction_on_image(\n",
    "        np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(output_overlay)\n",
    "    _ = plt.axis('off')\n",
    "\n",
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "\n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "\n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "\n",
    "    return angle\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'g',\n",
    "    (0, 2): 'g',\n",
    "    (1, 3): 'g',\n",
    "    (2, 4): 'g',\n",
    "    (0, 5): 'g',\n",
    "    (0, 6): 'g',\n",
    "    (5, 7): 'g',\n",
    "    (7, 9): 'g',\n",
    "    (6, 8): 'g',\n",
    "    (8, 10): 'g',\n",
    "    (5, 6): 'g',\n",
    "    (5, 11): 'g',\n",
    "    (6, 12): 'g',\n",
    "    (11, 12): 'g',\n",
    "    (11, 13): 'g',\n",
    "    (13, 15): 'g',\n",
    "    (12, 14): 'g',\n",
    "    (14, 16): 'g',\n",
    "}\n",
    "\n",
    "Downward_Facing_Dog_pose_or_Adho_Mukha_Svanasana_m = {\n",
    "    \"left_elbow_a\": 1,\n",
    "    \"right_elbow_a\": 1,\n",
    "    \"left_shoulder_b\": 0,\n",
    "    \"left_shoulder_a\": 0,\n",
    "    \"right_shoulder_b\": 0,\n",
    "    \"right_shoulder_a\": 0,\n",
    "    \"left_hip_a\": 0,\n",
    "    \"left_hip_b\": 0,\n",
    "    \"left_hip_c\": 0,\n",
    "    \"right_hip_a\": 0,\n",
    "    \"right_hip_b\": 0,\n",
    "    \"right_hip_c\": 0,\n",
    "    \"left_knee_a\": 1,\n",
    "    \"right_knee_a\": 1,\n",
    "}\n",
    "\n",
    "Tree_Pose_or_Vrksasana_m = {\n",
    "    \"left_elbow_a\": 0,\n",
    "    \"right_elbow_a\": 0,\n",
    "    \"left_shoulder_b\": 0,\n",
    "    \"left_shoulder_a\": 0,\n",
    "    \"right_shoulder_b\": 0,\n",
    "    \"right_shoulder_a\": 0,\n",
    "    \"left_hip_a\": 0,\n",
    "    \"left_hip_b\": 0,\n",
    "    \"left_hip_c\": 0,\n",
    "    \"right_hip_a\": 0,\n",
    "    \"right_hip_b\": 0,\n",
    "    \"right_hip_c\": 0,\n",
    "    \"left_knee_a\": 1,\n",
    "    \"right_knee_a\": 1,\n",
    "}\n",
    "\n",
    "Warrior_I_Pose_or_Virabhadrasana_I_m = {\n",
    "    \"left_elbow_a\": 1,\n",
    "    \"right_elbow_a\": 1,\n",
    "    \"left_shoulder_b\": 1,\n",
    "    \"left_shoulder_a\": 0,\n",
    "    \"right_shoulder_b\": 1,\n",
    "    \"right_shoulder_a\": 0,\n",
    "    \"left_hip_a\": 0,\n",
    "    \"left_hip_b\": 0,\n",
    "    \"left_hip_c\": 0,\n",
    "    \"right_hip_a\": 0,\n",
    "    \"right_hip_b\": 0,\n",
    "    \"right_hip_c\": 0,\n",
    "    \"left_knee_a\": 1,\n",
    "    \"right_knee_a\": 1,\n",
    "}\n",
    "\n",
    "Warrior_II_Pose_or_Virabhadrasana_II_m = {\n",
    "    \"left_elbow_a\": 1,\n",
    "    \"right_elbow_a\": 1,\n",
    "    \"left_shoulder_b\": 1,\n",
    "    \"left_shoulder_a\": 0,\n",
    "    \"right_shoulder_b\": 1,\n",
    "    \"right_shoulder_a\": 0,\n",
    "    \"left_hip_a\": 0,\n",
    "    \"left_hip_b\": 1,\n",
    "    \"left_hip_c\": 0,\n",
    "    \"right_hip_a\": 0,\n",
    "    \"right_hip_b\": 1,\n",
    "    \"right_hip_c\": 0,\n",
    "    \"left_knee_a\": 1,\n",
    "    \"right_knee_a\": 1,\n",
    "}\n",
    "\n",
    "Warrior_III_Pose_or_Virabhadrasana_III_m = {\n",
    "    \"left_elbow_a\": 1,\n",
    "    \"right_elbow_a\": 1,\n",
    "    \"left_shoulder_b\": 1,\n",
    "    \"left_shoulder_a\": 0,\n",
    "    \"right_shoulder_b\": 1,\n",
    "    \"right_shoulder_a\": 0,\n",
    "    \"left_hip_a\": 0,\n",
    "    \"left_hip_b\": 1,\n",
    "    \"left_hip_c\": 0,\n",
    "    \"right_hip_a\": 0,\n",
    "    \"right_hip_b\": 1,\n",
    "    \"right_hip_c\": 0,\n",
    "    \"left_knee_a\": 1,\n",
    "    \"right_knee_a\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../images'\n",
    "image_path = root_dir + '/' + 'test2.jpg'\n",
    "image = np.array(Image.open(image_path))\n",
    "\n",
    "prediction = 'Downward_Facing_Dog_pose_or_Adho_Mukha_Svanasana_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447, 640, 3)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def pose_detection_model(image, prediction):\n",
    "\n",
    "#decode_image = tf.image.decode_jpeg(image)\n",
    "input_image = tf.expand_dims(image, axis=0)\n",
    "input_size = 192\n",
    "input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "    \n",
    "def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "\n",
    "    model = module.signatures['serving_default']\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    \n",
    "    outputs = model(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints_with_scores = outputs['output_0'].numpy()\n",
    "    \n",
    "    return keypoints_with_scores\n",
    "\n",
    "keypoints_with_scores = movenet(input_image)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_with_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_calc(keypoints_with_scores):  #(image_capture)\n",
    "    # input_size = 192\n",
    "    # Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "    # input_image = tf.expand_dims(image, axis=0)\n",
    "    # input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "    # Run model inference.\n",
    "    # keypoints_with_scores = movenet(input_image)\n",
    "\n",
    "    # # Visualize the predictions with image.\n",
    "    # display_image = tf.expand_dims(image, axis=0)\n",
    "    # display_image = tf.cast(tf.image.resize_with_pad(\n",
    "    #     display_image, 1280, 1280), dtype=tf.int32)\n",
    "    # output_overlay = draw_prediction_on_image(\n",
    "    #     np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "\n",
    "    key_xy = keypoints_with_scores[:, :, :, :2]\n",
    "\n",
    "    # Create a dictionary of keypoints and their corresponding vector tuples\n",
    "    key_dict = {}\n",
    "    for key, value in KEYPOINT_DICT.items():\n",
    "        vector = tuple(key_xy[0, 0, value])\n",
    "        key_dict[key] = vector\n",
    "\n",
    "\n",
    "    # make angles_dictionary\n",
    "    angles_dictionary = {\n",
    "    \"left_elbow_a\": (key_dict[\"left_wrist\"],key_dict[\"left_elbow\"],key_dict[\"left_shoulder\"]),\n",
    "    \"right_elbow_a\": (key_dict[\"right_wrist\"],key_dict[\"right_elbow\"],key_dict[\"right_shoulder\"]),\n",
    "    \"left_shoulder_b\": (key_dict[\"left_elbow\"],key_dict[\"left_shoulder\"],key_dict[\"left_hip\"]),\n",
    "    \"left_shoulder_a\": (key_dict[\"left_hip\"],key_dict[\"left_shoulder\"],key_dict[\"right_shoulder\"]),\n",
    "    \"right_shoulder_b\": (key_dict[\"right_elbow\"],key_dict[\"right_shoulder\"],key_dict[ \"right_hip\"]),\n",
    "    \"right_shoulder_a\": (key_dict[\"right_hip\"],key_dict[\"right_shoulder\"],key_dict[ \"left_shoulder\"]),\n",
    "    \"left_hip_a\": (key_dict[\"left_shoulder\"],key_dict[\"left_hip\"],key_dict[ \"right_hip\"]),\n",
    "    \"left_hip_b\": (key_dict[\"left_shoulder\"],key_dict[\"left_hip\"],key_dict[ \"left_knee\"]),\n",
    "    \"left_hip_c\": (key_dict[\"right_hip\"],key_dict[\"left_hip\"],key_dict[ \"left_knee\"]),\n",
    "    \"right_hip_a\": (key_dict[\"right_shoulder\"],key_dict[\"right_hip\"],key_dict[ \"left_hip\"]),\n",
    "    \"right_hip_b\": (key_dict[\"left_hip\"],key_dict[\"right_hip\"],key_dict[ \"right_knee\"]),\n",
    "    \"right_hip_c\": (key_dict[\"right_shoulder\"],key_dict[\"right_hip\"],key_dict[ \"right_knee\"]),\n",
    "    \"left_knee_a\": (key_dict[\"left_hip\"],key_dict[\"left_knee\"],key_dict[ \"left_ankle\"]),\n",
    "    \"right_knee_a\": (key_dict[\"right_hip\"],key_dict[\"right_knee\"],key_dict[ \"right_ankle\"])\n",
    "    }\n",
    "\n",
    "    angles = {}\n",
    "\n",
    "    for key, value in angles_dictionary.items():\n",
    "        angle = calculate_angle(value[0], value[1], value[2])\n",
    "        angles[key] = angle\n",
    "\n",
    "    return(angles)\n",
    "\n",
    "\n",
    "\n",
    "angles = angle_calc(keypoints_with_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get angles comparrison\n",
    "def compare_angles(prediction, angles):\n",
    "\n",
    "    # Opening JSON file\n",
    "    with open('data.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "        # Print the type of data variable\n",
    "        #print(\"Type:\", type(data))\n",
    "\n",
    "        # Print the data of dictionary\n",
    "        ground_pose_dict = data\n",
    "\n",
    "    dict1 = ground_pose_dict[prediction]\n",
    "    dict2 = angles\n",
    "    dict3 = {}\n",
    "\n",
    "    x = str(prediction + 'm')\n",
    "    dict4 = eval(x)\n",
    "\n",
    "    for dict1_key, dict1_values in dict1.items():\n",
    "        dict3[dict1_key] = abs(dict2[dict1_key]-dict1[dict1_key]) * dict4[dict1_key]\n",
    "\n",
    "    return dict3\n",
    "\n",
    "dict3 = compare_angles(prediction, angles)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_red(dict3, KEYPOINT_EDGE_INDS_TO_COLOR):\n",
    "    '''\n",
    "    Takes the dict of differences in angles and looks for an angle difference more than 10 degrees,\n",
    "    then changes the colours of the corresponding bars around the angle to red.\n",
    "    '''\n",
    "    bars_dictionary = {\n",
    "        \"left_elbow_a\": [(5,7),(7,9)],\n",
    "        \"right_elbow_a\": [(6,8),(8,10)],\n",
    "        \"left_shoulder_b\": [(5,7),(5,11)],\n",
    "        \"left_shoulder_a\": [(5,6),(5,11)],\n",
    "        \"right_shoulder_b\": [(6,8),(6,12)],\n",
    "        \"right_shoulder_a\": [(5,6),(6,12)],\n",
    "        \"left_hip_a\": [(5,11),(11,12)],\n",
    "        \"left_hip_b\": [(11,12),(11,13)],\n",
    "        \"left_hip_c\": [(5,11),(11,13)],\n",
    "        \"right_hip_a\": [(6,12),(11,12)],\n",
    "        \"right_hip_b\": [(11,12),(12,14)],\n",
    "        \"right_hip_c\": [(6,12),(12,14)],\n",
    "        \"left_knee_a\": [(11,13),(13,15)],\n",
    "        \"right_knee_a\": [(12,14),(14,16)]\n",
    "    }\n",
    "\n",
    "    RED_EDGES = KEYPOINT_EDGE_INDS_TO_COLOR.copy()\n",
    "\n",
    "    for k, v in dict3.items():\n",
    "        if v >= 5:\n",
    "            points_to_color = bars_dictionary[k]\n",
    "            for point in points_to_color:\n",
    "                RED_EDGES[point] = 'r'\n",
    "\n",
    "    return RED_EDGES\n",
    "\n",
    "RED_EDGES = render_red(dict3, KEYPOINT_EDGE_INDS_TO_COLOR)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RED_EDGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = image.shape[0]\n",
    "width = image.shape[1]\n",
    "\n",
    "def _keypoints_and_edges_for_display_red(keypoints_with_scores,\n",
    "                                        RED_EDGES,\n",
    "                                        height,\n",
    "                                        width,\n",
    "                                        keypoint_threshold=0.11):\n",
    "\n",
    "    \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "    Args:\n",
    "        keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "        the keypoint coordinates and scores returned from the MoveNet model.\n",
    "        height: height of the image in pixels.\n",
    "        width: width of the image in pixels.\n",
    "        keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "        visualized.\n",
    "\n",
    "    Returns:\n",
    "        A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "        * the coordinates of all keypoints of all detected entities;\n",
    "        * the coordinates of all skeleton edges of all detected entities;\n",
    "        * the colors in which the edges should be plotted.\n",
    "    \"\"\"\n",
    "    keypoints_all = []\n",
    "    keypoint_edges_all = []\n",
    "    edge_colors = []\n",
    "    num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "    for idx in range(num_instances):\n",
    "        kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "        kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "        kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "        kpts_absolute_xy = np.stack(\n",
    "            [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "        kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "            kpts_scores > keypoint_threshold, :]\n",
    "        keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in RED_EDGES.items():\n",
    "        if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "            kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "            x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "            y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "            x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "            y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "            line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "            keypoint_edges_all.append(line_seg)\n",
    "            edge_colors.append(color)\n",
    "    if keypoints_all:\n",
    "        keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "    else:\n",
    "        keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "    if keypoint_edges_all:\n",
    "        edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "    else:\n",
    "        edges_xy = np.zeros((0, 2, 2))\n",
    "    return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "keypoints_xy, edges_xy, edge_colors = _keypoints_and_edges_for_display_red(keypoints_with_scores,\n",
    "                                                                           RED_EDGES,\n",
    "                                                    height=height,\n",
    "                                                    width=width,\n",
    "                                                    keypoint_threshold=0.11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_prediction_on_image_red(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "    \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "    Args:\n",
    "        image: A numpy array with shape [height, width, channel] representing the\n",
    "        pixel values of the input image.\n",
    "        keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "        the keypoint coordinates and scores returned from the MoveNet model.\n",
    "        crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "        of the crop region in normalized coordinates (see the init_crop_region\n",
    "        function below for more detail). If provided, this function will also\n",
    "        draw the bounding box on the image.\n",
    "        output_image_height: An integer indicating the height of the output image.\n",
    "        Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array with shape [out_height, out_width, channel] representing the\n",
    "        image overlaid with keypoint predictions.\n",
    "    \"\"\"\n",
    "    height, width, channel = image.shape\n",
    "    aspect_ratio = float(width) / height\n",
    "    fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "    # To remove the huge white borders\n",
    "    fig.tight_layout(pad=0)\n",
    "    ax.margins(0)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    plt.axis('off')\n",
    "\n",
    "    im = ax.imshow(image)\n",
    "    line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "    ax.add_collection(line_segments)\n",
    "    # Turn off tick labels\n",
    "    scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "    (keypoint_locs, keypoint_edges,\n",
    "    edge_colors) = _keypoints_and_edges_for_display_red(keypoints_with_scores, RED_EDGES,\n",
    "                                                    height=height,\n",
    "                                                    width=width,\n",
    "                                                    keypoint_threshold=0.11)\n",
    "\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "    if keypoint_edges.shape[0]:\n",
    "        line_segments.set_segments(keypoint_edges)\n",
    "        line_segments.set_color(edge_colors)\n",
    "    if keypoint_locs.shape[0]:\n",
    "        scat.set_offsets(keypoint_locs)\n",
    "\n",
    "    if crop_region is not None:\n",
    "        xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "        ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "        rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "        rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin,ymin),rec_width,rec_height,\n",
    "            linewidth=1,edgecolor='b',facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    image_from_plot = image_from_plot.reshape(\n",
    "        fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    plt.close(fig)\n",
    "    if output_image_height is not None:\n",
    "        output_image_width = int(output_image_height / height * width)\n",
    "        image_from_plot = cv2.resize(\n",
    "            image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "            interpolation=cv2.INTER_CUBIC)\n",
    "    return image_from_plot\n",
    "\n",
    "image_from_plot = draw_prediction_on_image_red(image,\n",
    "                                keypoints_with_scores,\n",
    "                                crop_region=None,\n",
    "                                close_figure=False,\n",
    "                                output_image_height=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_red(keypoints_with_scores,image):\n",
    "\n",
    "    # Visualize the predictions with image.\n",
    "    display_image = tf.expand_dims(image, axis=0)\n",
    "    display_image = tf.cast(tf.image.resize_with_pad(\n",
    "        display_image, 1280, 1280), dtype=tf.int32)\n",
    "    output_overlay = draw_prediction_on_image_red(\n",
    "        np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "\n",
    "    # plt.figure(figsize=(5, 5))\n",
    "    # plt.imshow(output_overlay)\n",
    "    # _ = plt.axis('off')\n",
    "    return output_overlay\n",
    "\n",
    "output_overlay = plot_red(keypoints_with_scores,image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_overlay = pose_detection_model(image, prediction)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(output_overlay)\n",
    "_ = plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yogi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
