{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73faf589",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /home/dolly/.pyenv/versions/3.10.6/envs/YOGi/lib/python3.10/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/dolly/.pyenv/versions/3.10.6/envs/YOGi/lib/python3.10/site-packages (from opencv-python) (1.24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opencv-python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adccf1c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "787d6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import utils\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbea69",
   "metadata": {},
   "source": [
    "# Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5a00e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7353e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_list = os.listdir(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6da790e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pose_1', 'Pose_2']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0492539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_train_val_dirs\n",
    "def create_train_val_dirs(root_path):\n",
    "    for pose in poses_list:\n",
    "        if pose == \"test\" or pose == \"training\":\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(os.path.join(root_path, 'training', pose))\n",
    "            os.makedirs(os.path.join(root_path, 'test', pose))\n",
    "    \n",
    "try:\n",
    "    create_train_val_dirs(root_path=root_dir)\n",
    "except FileExistsError:\n",
    "    print(\"You should not be seeing this since the upper directory is removed beforehand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d900cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
    "    \"\"\"Splits the data into train and test sets\"\"\"\n",
    "    \n",
    "    source_images = [image for image in os.listdir(SOURCE_DIR)]\n",
    "    # Randomising list\n",
    "    source_images = random.sample(source_images, len(source_images))\n",
    "    \n",
    "    train_images = source_images[:int(SPLIT_SIZE * len(source_images))]\n",
    "    val_images = source_images[int(SPLIT_SIZE * len(source_images)):]\n",
    "    for image in train_images:\n",
    "        shutil.copyfile(os.path.join(SOURCE_DIR, image), os.path.join(TRAINING_DIR, image))\n",
    "    for image in val_images:\n",
    "        shutil.copyfile(os.path.join(SOURCE_DIR, image), os.path.join(VALIDATION_DIR, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f976f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 images for Pose_1 in training\n",
      "There are 3 images for Pose_1 in testing\n",
      "Original Pose_1's directory has 8 images\n",
      "\n",
      "There are 5 images for Pose_2 in training\n",
      "There are 3 images for Pose_2 in testing\n",
      "Original Pose_2's directory has 8 images\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_size = 0.66\n",
    "\n",
    "for pose in poses_list:\n",
    "    SOURCE_DIR = f\"{root_dir}/{pose}\"\n",
    "    TRAINING_DIR = f\"{root_dir}/training/{pose}\"\n",
    "    TESTING_DIR = f\"{root_dir}/test/{pose}\"    \n",
    "    split_data(SOURCE_DIR, TRAINING_DIR, TESTING_DIR, split_size)\n",
    "    \n",
    "    print(f\"There are {len(os.listdir(TRAINING_DIR))} images for {pose} in training\")\n",
    "    print(f\"There are {len(os.listdir(TESTING_DIR))} images for {pose} in testing\")\n",
    "    \n",
    "    print(f\"Original {pose}'s directory has {len(os.listdir(SOURCE_DIR))} images\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e20bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../raw_data/training\"\n",
    "img_height, img_width = 100, 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4be94b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 images belonging to 2 classes.\n",
      "Found 2 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True,\n",
    "                                    validation_split=0.2) # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                    train_dir,\n",
    "                                    target_size=(img_height, img_width),\n",
    "                                    batch_size=batch_size,\n",
    "                                    class_mode='binary',\n",
    "                                    subset='training') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "                                    train_dir, # same directory as training data\n",
    "                                    target_size=(img_height, img_width),\n",
    "                                    batch_size=batch_size,\n",
    "                                    class_mode='binary',\n",
    "                                    subset='validation') # set as validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96e7f771",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 images belonging to 2 classes.\n",
      "Found 6 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.preprocessing.image.DirectoryIterator at 0x7fa7e0521210>,\n",
       " <keras.preprocessing.image.DirectoryIterator at 0x7fa88beda0b0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_generators(\"../raw_data/training\", \"../raw_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f530f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4675f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# Do model architecture and compile \n",
    "# DUMMY MODEL!!!!!!!!!\n",
    "...\n",
    "def create_model():\n",
    "    model = models.Sequential([ \n",
    "        layers.Conv2D(16, (3,3), activation='relu', input_shape = (150, 150, 3)),\n",
    "        layers.MaxPooling2D(2,2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation = 'relu'),\n",
    "        layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='multiclass_crossentropy',\n",
    "                metrics=['accuracy']) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fb61e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6636f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/YOGi/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/YOGi/lib/python3.10/site-packages/keras/engine/training.py:1662\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1660\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1662\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1663\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected result of `train_function` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1664\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Empty logs). Please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1665\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1666\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1667\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformation of where went wrong, or file a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1668\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue/bug to `tf.keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1669\u001b[0m     )\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_get_metrics_result(logs)\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // batch_size,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // batch_size,\n",
    "    epochs = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3784a400",
   "metadata": {},
   "source": [
    "# Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread(f\"{file_path}/{file_list[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec4d62e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Detected :  2\n"
     ]
    }
   ],
   "source": [
    "# Reading the Image\n",
    "image = cv2.imread(f\"{file_path}/{file_list[3]}\")\n",
    "\n",
    "# initialize the HOG descriptor\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "(humans, _) = hog.detectMultiScale(image, winStride=(3,3),\n",
    "                                padding=(32, 32), \n",
    "                               scale=1.01)\n",
    "\n",
    "# getting no. of human detected\n",
    "print('Human Detected : ', len(humans))\n",
    "\n",
    "# loop over all detected humans\n",
    "for (x, y, w, h) in humans:\n",
    "   pad_w, pad_h = int(0.15 * w), int(0.01 * h)\n",
    "   cv2.rectangle(image, (x + pad_w, y + pad_h), (x + w - pad_w, y + h - pad_h), (0, 255, 0), 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1bf615f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the output image\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(25)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07339aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "170031fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"{file_path}/{file_list[3]}\"\n",
    "photo = cv2.imread(filepath)\n",
    "photo_grey = cv2.cvtColor(photo, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06134bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the HOG person\n",
    "# detector\n",
    "\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "   \n",
    "# Reading the Image\n",
    "image = cv2.imread(filepath)\n",
    "   \n",
    "# Detecting all the regions in the \n",
    "# Image that has a pedestrians inside it\n",
    "(regions, _) = hog.detectMultiScale(image, \n",
    "                                    winStride=(8, 8),\n",
    "                                    padding=(16, 16),\n",
    "                                    scale=1.15)\n",
    "   \n",
    "# Drawing the regions in the Image\n",
    "for (x, y, w, h) in regions:\n",
    "    cv2.rectangle(image, (x, y), \n",
    "                  (x + w, y + h), \n",
    "                  (0, 0, 255), 2)\n",
    "    \n",
    "#Showing the output Image\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)\n",
    "   \n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
