{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73faf589",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /home/dolly/.pyenv/versions/3.10.6/envs/YOGi/lib/python3.10/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/dolly/.pyenv/versions/3.10.6/envs/YOGi/lib/python3.10/site-packages (from opencv-python) (1.24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opencv-python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adccf1c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "787d6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import utils\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbea69",
   "metadata": {},
   "source": [
    "# Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7353e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Garland_Pose',\n",
       " 'Peacock_Pose',\n",
       " 'Four-Limbed_Staff',\n",
       " 'Sitting_pose_1_(normal)',\n",
       " 'Staff_Pose_',\n",
       " 'Low_Lunge_pose',\n",
       " 'Standing_big_toe_hold']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '../raw_data'\n",
    "poses_list = os.listdir(root_dir) #Lists all folders in the root directory (raw_data folder)\n",
    "poses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0492539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates Testing and Training directories\n",
    "def create_train_val_dirs(root_path):\n",
    "    for pose in poses_list:\n",
    "        if pose == \"Testing\" or pose == \"Training\":\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(os.path.join(root_path, 'Training', pose))\n",
    "            os.makedirs(os.path.join(root_path, 'Testing', pose))\n",
    "            \n",
    "            \n",
    "#Tries to create new directories\n",
    "#Errors if Training/Testing directories already exist - delete them before running this notebook!\n",
    "try:\n",
    "    create_train_val_dirs(root_path=root_dir)\n",
    "except FileExistsError:\n",
    "    print(\"You should not be seeing this since the upper directory is removed beforehand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d900cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
    "    \"\"\"Function to split the data into train/test\n",
    "    First checks that the image exists in the folder - this is a safety net for any images which don't exist that we missed when manually checking images \n",
    "    Creates a list of all images and randomises them\n",
    "    Splits into train test depending on split size (defined below)\n",
    "    Copies all the train images into train folder and vice versa\n",
    "    \"\"\"\n",
    "    \n",
    "    ignore = []\n",
    "    for image in os.listdir(SOURCE_DIR):\n",
    "        if type(cv2.imread(os.path.join(SOURCE_DIR, image))) is type(None):\n",
    "            ignore.append(image)\n",
    "    \n",
    "    source_images = [image for image in os.listdir(SOURCE_DIR) if image not in ignore]\n",
    "    # Randomising list\n",
    "    source_images = random.sample(source_images, len(source_images))\n",
    "    \n",
    "    train_images = source_images[:int(SPLIT_SIZE * len(source_images))]\n",
    "    val_images = source_images[int(SPLIT_SIZE * len(source_images)):]\n",
    "    for image in train_images:\n",
    "        shutil.copyfile(os.path.join(SOURCE_DIR, image), os.path.join(TRAINING_DIR, image))\n",
    "    for image in val_images:\n",
    "        shutil.copyfile(os.path.join(SOURCE_DIR, image), os.path.join(VALIDATION_DIR, image))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f976f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 147 images for Garland_Pose in training\n",
      "There are 64 images for Garland_Pose in testing\n",
      "Original Garland_Pose's directory has 236 images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 79 images for Peacock_Pose in training\n",
      "There are 34 images for Peacock_Pose in testing\n",
      "Original Peacock_Pose's directory has 177 images\n",
      "\n",
      "There are 137 images for Four-Limbed_Staff in training\n",
      "There are 59 images for Four-Limbed_Staff in testing\n",
      "Original Four-Limbed_Staff's directory has 203 images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "Corrupt JPEG data: 21 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 422 images for Sitting_pose_1_(normal) in training\n",
      "There are 182 images for Sitting_pose_1_(normal) in testing\n",
      "Original Sitting_pose_1_(normal)'s directory has 637 images\n",
      "\n",
      "There are 49 images for Staff_Pose_ in training\n",
      "There are 21 images for Staff_Pose_ in testing\n",
      "Original Staff_Pose_'s directory has 72 images\n",
      "\n",
      "There are 163 images for Low_Lunge_pose in training\n",
      "There are 71 images for Low_Lunge_pose in testing\n",
      "Original Low_Lunge_pose's directory has 295 images\n",
      "\n",
      "There are 117 images for Standing_big_toe_hold in training\n",
      "There are 51 images for Standing_big_toe_hold in testing\n",
      "Original Standing_big_toe_hold's directory has 228 images\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_size = 0.7\n",
    "\n",
    "#Loops over all the poses, defines the directories for source/train/test\n",
    "#Splits the data into train/test\n",
    "#Prints how many images are in train/test and in the original directory - sanity check!\n",
    "\n",
    "for pose in poses_list:\n",
    "    if pose == \"Training\" or pose == \"Testing\":\n",
    "        pass\n",
    "    else:\n",
    "        SOURCE_DIR = f\"{root_dir}/{pose}\"\n",
    "        TRAINING_DIR = f\"{root_dir}/Training/{pose}\"\n",
    "        TESTING_DIR = f\"{root_dir}/Testing/{pose}\"    \n",
    "        split_data(SOURCE_DIR, TRAINING_DIR, TESTING_DIR, split_size)\n",
    "\n",
    "        print(f\"There are {len(os.listdir(TRAINING_DIR))} images for {pose} in training\")\n",
    "        print(f\"There are {len(os.listdir(TESTING_DIR))} images for {pose} in testing\")\n",
    "\n",
    "        print(f\"Original {pose}'s directory has {len(os.listdir(SOURCE_DIR))} images\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e20bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These variables can be changes, excluding train_dir\n",
    "\n",
    "train_dir = \"../raw_data/Training\"\n",
    "img_height, img_width = 256, 256\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4be94b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 895 images belonging to 7 classes.\n",
      "Found 219 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "#Splits into train_generator and validation_generator\n",
    "#This bulk uploads the images\n",
    "#Creates target (y) for us!\n",
    "\n",
    "#Play around with the interpolation argument - bicubic, lanczos??? \n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                    vertical_flip=True,\n",
    "                                    validation_split=0.2) # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                    train_dir,\n",
    "                                    target_size=(img_height, img_width),\n",
    "                                    batch_size=batch_size,\n",
    "                                    class_mode='categorical',\n",
    "                                    subset='training',\n",
    "                                    keep_aspect_ratio=True,\n",
    "                                    interpolation='lanczos') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "                                    train_dir, # same directory as training data\n",
    "                                    target_size=(img_height, img_width),\n",
    "                                    batch_size=batch_size,\n",
    "                                    class_mode='categorical',\n",
    "                                    subset='validation',\n",
    "                                    keep_aspect_ratio=True,\n",
    "                                    interpolation='lanczos') # set as validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f530f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4675f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# Model needs building + transfer learning  \n",
    "...\n",
    "def instantiate_model():\n",
    "    model = models.Sequential([ \n",
    "        layers.Conv2D(16, (3,3), activation='relu', input_shape = (256, 256,3)),\n",
    "        layers.MaxPooling2D(2,2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation = 'relu'),\n",
    "        layers.Dense(7, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy']) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fb61e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 12:36:37.398480: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dolly/.pyenv/versions/3.10.6/envs/YOGi/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-03-08 12:36:37.399484: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-08 12:36:37.399689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Dolly): /proc/driver/nvidia/version does not exist\n",
      "2023-03-08 12:36:37.403291: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = instantiate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6636f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dolly/.pyenv/versions/3.10.6/envs/YOGi/lib/python3.10/site-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "27/27 [==============================] - 44s 2s/step - loss: 24.1407 - accuracy: 0.2410 - val_loss: 2.2288 - val_accuracy: 0.2292\n",
      "Epoch 2/2\n",
      "27/27 [==============================] - 31s 1s/step - loss: 1.7675 - accuracy: 0.4426 - val_loss: 1.8093 - val_accuracy: 0.4219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb8530b95a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model - fit on train_generator (both X and y) and the validation data is validation_generator\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // batch_size,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // batch_size,\n",
    "    epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3784a400",
   "metadata": {},
   "source": [
    "# Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has been paused and probably won't be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread(f\"{file_path}/{file_list[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec4d62e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Detected :  2\n"
     ]
    }
   ],
   "source": [
    "# Reading the Image\n",
    "image = cv2.imread(f\"{file_path}/{file_list[3]}\")\n",
    "\n",
    "# initialize the HOG descriptor\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "(humans, _) = hog.detectMultiScale(image, winStride=(3,3),\n",
    "                                padding=(32, 32), \n",
    "                               scale=1.01)\n",
    "\n",
    "# getting no. of human detected\n",
    "print('Human Detected : ', len(humans))\n",
    "\n",
    "# loop over all detected humans\n",
    "for (x, y, w, h) in humans:\n",
    "   pad_w, pad_h = int(0.15 * w), int(0.01 * h)\n",
    "   cv2.rectangle(image, (x + pad_w, y + pad_h), (x + w - pad_w, y + h - pad_h), (0, 255, 0), 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1bf615f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the output image\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(25)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07339aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "170031fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"{file_path}/{file_list[3]}\"\n",
    "photo = cv2.imread(filepath)\n",
    "photo_grey = cv2.cvtColor(photo, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06134bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the HOG person\n",
    "# detector\n",
    "\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "   \n",
    "# Reading the Image\n",
    "image = cv2.imread(filepath)\n",
    "   \n",
    "# Detecting all the regions in the \n",
    "# Image that has a pedestrians inside it\n",
    "(regions, _) = hog.detectMultiScale(image, \n",
    "                                    winStride=(8, 8),\n",
    "                                    padding=(16, 16),\n",
    "                                    scale=1.15)\n",
    "   \n",
    "# Drawing the regions in the Image\n",
    "for (x, y, w, h) in regions:\n",
    "    cv2.rectangle(image, (x, y), \n",
    "                  (x + w, y + h), \n",
    "                  (0, 0, 255), 2)\n",
    "    \n",
    "#Showing the output Image\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)\n",
    "   \n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
