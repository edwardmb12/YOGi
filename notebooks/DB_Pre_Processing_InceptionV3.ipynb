{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73faf589",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install opencv-python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adccf1c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787d6a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 16:03:24.307150: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-08 16:03:24.723285: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-08 16:03:24.795910: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dolly/.pyenv/versions/3.10.6/envs/YOGi/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-03-08 16:03:24.795933: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-08 16:03:26.757014: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dolly/.pyenv/versions/3.10.6/envs/YOGi/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-03-08 16:03:26.757208: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dolly/.pyenv/versions/3.10.6/envs/YOGi/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-03-08 16:03:26.757219: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import utils, optimizers\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.applications import inception_v3\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbea69",
   "metadata": {},
   "source": [
    "# Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7353e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../raw_data'\n",
    "poses_list = os.listdir(root_dir) #Lists all folders in the root directory (raw_data folder)\n",
    "poses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0492539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates Testing and Training directories\n",
    "def create_train_val_dirs(root_path):\n",
    "    for pose in poses_list:\n",
    "        if pose == \"Testing\" or pose == \"Training\":\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(os.path.join(root_path, 'Training', pose))\n",
    "            os.makedirs(os.path.join(root_path, 'Testing', pose))\n",
    "            \n",
    "            \n",
    "#Tries to create new directories\n",
    "#Errors if Training/Testing directories already exist - delete them before running this notebook!\n",
    "try:\n",
    "    create_train_val_dirs(root_path=root_dir)\n",
    "except FileExistsError:\n",
    "    print(\"You should not be seeing this since the upper directory is removed beforehand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d900cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
    "    \"\"\"Function to split the data into train/test\n",
    "    First checks that the image exists in the folder - this is a safety net for any images which don't exist that we missed when manually checking images \n",
    "    Creates a list of all images and randomises them\n",
    "    Splits into train test depending on split size (defined below)\n",
    "    Copies all the train images into train folder and vice versa\n",
    "    \"\"\"\n",
    "    \n",
    "    ignore = []\n",
    "    for image in os.listdir(SOURCE_DIR):\n",
    "        if type(cv2.imread(os.path.join(SOURCE_DIR, image))) is type(None):\n",
    "            ignore.append(image)\n",
    "    \n",
    "    source_images = [image for image in os.listdir(SOURCE_DIR) if image not in ignore]\n",
    "    # Randomising list\n",
    "    source_images = random.sample(source_images, len(source_images))\n",
    "    \n",
    "    train_images = source_images[:int(SPLIT_SIZE * len(source_images))]\n",
    "    val_images = source_images[int(SPLIT_SIZE * len(source_images)):]\n",
    "    for image in train_images:\n",
    "        shutil.copyfile(os.path.join(SOURCE_DIR, image), os.path.join(TRAINING_DIR, image))\n",
    "    for image in val_images:\n",
    "        shutil.copyfile(os.path.join(SOURCE_DIR, image), os.path.join(VALIDATION_DIR, image))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f976f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = 0.7\n",
    "\n",
    "#Loops over all the poses, defines the directories for source/train/test\n",
    "#Splits the data into train/test\n",
    "#Prints how many images are in train/test and in the original directory - sanity check!\n",
    "\n",
    "for pose in poses_list:\n",
    "    if pose == \"Training\" or pose == \"Testing\":\n",
    "        pass\n",
    "    else:\n",
    "        SOURCE_DIR = f\"{root_dir}/{pose}\"\n",
    "        TRAINING_DIR = f\"{root_dir}/Training/{pose}\"\n",
    "        TESTING_DIR = f\"{root_dir}/Testing/{pose}\"    \n",
    "        split_data(SOURCE_DIR, TRAINING_DIR, TESTING_DIR, split_size)\n",
    "\n",
    "        print(f\"There are {len(os.listdir(TRAINING_DIR))} images for {pose} in training\")\n",
    "        print(f\"There are {len(os.listdir(TESTING_DIR))} images for {pose} in testing\")\n",
    "\n",
    "        print(f\"Original {pose}'s directory has {len(os.listdir(SOURCE_DIR))} images\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e20bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These variables can be changes, excluding train_dir\n",
    "\n",
    "train_dir = \"../raw_data/Training\"\n",
    "img_height, img_width = 256, 256\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be94b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits into train_generator and validation_generator\n",
    "#This bulk uploads the images\n",
    "#Creates target (y) for us!\n",
    "\n",
    "#Play around with the interpolation argument - bicubic, lanczos??? \n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                    vertical_flip=True,\n",
    "                                    validation_split=0.2) # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                    train_dir,\n",
    "                                    target_size=(img_height, img_width),\n",
    "                                    batch_size=batch_size,\n",
    "                                    class_mode='categorical',\n",
    "                                    subset='training',\n",
    "                                    keep_aspect_ratio=True,\n",
    "                                    interpolation='lanczos') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "                                    train_dir, # same directory as training data\n",
    "                                    target_size=(img_height, img_width),\n",
    "                                    batch_size=batch_size,\n",
    "                                    class_mode='categorical',\n",
    "                                    subset='validation',\n",
    "                                    keep_aspect_ratio=True,\n",
    "                                    interpolation='lanczos') # set as validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f530f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4675f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# Model needs building + transfer learning  \n",
    "...\n",
    "def initialize_model():\n",
    "    base_model = inception_v3.InceptionV3(\n",
    "                        include_top=False,\n",
    "                        weights='imagenet',\n",
    "                        input_shape=(img_height, img_width, 3),\n",
    "                        classifier_activation='softmax')\n",
    "    \n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model = models.Sequential([ \n",
    "        base_model,\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(500, activation=\"relu\"),\n",
    "        layers.Dense(7, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    opt = optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy']) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb61e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63383da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d35456",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636f75f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fit model - fit on train_generator (both X and y) and the validation data is validation_generator\n",
    "history = model.fit(\n",
    "                train_generator,\n",
    "                steps_per_epoch = train_generator.samples // batch_size,\n",
    "                validation_data = validation_generator, \n",
    "                validation_steps = validation_generator.samples // batch_size,\n",
    "                epochs = 4,\n",
    "                callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ec46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title='', axs=None, exp_name=\"\"):\n",
    "    if axs is not None:\n",
    "        ax1, ax2 = axs\n",
    "    else:\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    if len(exp_name) > 0 and exp_name[0] != '_':\n",
    "        exp_name = '_' + exp_name\n",
    "    ax1.plot(history.history['loss'], label='train' + exp_name)\n",
    "    ax1.plot(history.history['val_loss'], label='val' + exp_name)\n",
    "    #ax1.set_ylim(0., 2.2)\n",
    "    ax1.set_title('loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history.history['accuracy'], label='train accuracy'  + exp_name)\n",
    "    ax2.plot(history.history['val_accuracy'], label='val accuracy'  + exp_name)\n",
    "    #ax2.set_ylim(0.25, 1.)\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.legend()\n",
    "    return (ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a245ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = f\"{root_dir}/Testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255) # set validation split\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                                    test_dir,\n",
    "                                    target_size=(img_height, img_width),\n",
    "                                    batch_size=batch_size,\n",
    "                                    class_mode='categorical',\n",
    "                                    subset='training',\n",
    "                                    keep_aspect_ratio=True,\n",
    "                                    interpolation='lanczos') # set as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a49c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3784a400",
   "metadata": {},
   "source": [
    "# Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has been paused and probably won't be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread(f\"{file_path}/{file_list[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d62e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Image\n",
    "image = cv2.imread(f\"{file_path}/{file_list[3]}\")\n",
    "\n",
    "# initialize the HOG descriptor\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "(humans, _) = hog.detectMultiScale(image, winStride=(3,3),\n",
    "                                padding=(32, 32), \n",
    "                               scale=1.01)\n",
    "\n",
    "# getting no. of human detected\n",
    "print('Human Detected : ', len(humans))\n",
    "\n",
    "# loop over all detected humans\n",
    "for (x, y, w, h) in humans:\n",
    "   pad_w, pad_h = int(0.15 * w), int(0.01 * h)\n",
    "   cv2.rectangle(image, (x + pad_w, y + pad_h), (x + w - pad_w, y + h - pad_h), (0, 255, 0), 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bf615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the output image\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(25)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07339aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170031fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"{file_path}/{file_list[3]}\"\n",
    "photo = cv2.imread(filepath)\n",
    "photo_grey = cv2.cvtColor(photo, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06134bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the HOG person\n",
    "# detector\n",
    "\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "   \n",
    "# Reading the Image\n",
    "image = cv2.imread(filepath)\n",
    "   \n",
    "# Detecting all the regions in the \n",
    "# Image that has a pedestrians inside it\n",
    "(regions, _) = hog.detectMultiScale(image, \n",
    "                                    winStride=(8, 8),\n",
    "                                    padding=(16, 16),\n",
    "                                    scale=1.15)\n",
    "   \n",
    "# Drawing the regions in the Image\n",
    "for (x, y, w, h) in regions:\n",
    "    cv2.rectangle(image, (x, y), \n",
    "                  (x + w, y + h), \n",
    "                  (0, 0, 255), 2)\n",
    "    \n",
    "#Showing the output Image\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)\n",
    "   \n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
